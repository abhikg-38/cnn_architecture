{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30b5749c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of images:\n",
      "2\n",
      "Shape of images:\n",
      "(2, 427, 640, 3)\n",
      "Shape of images after preprocessing:\n",
      "(2, 70, 120, 3)\n",
      "Shape of created feature maps before zero padding:\n",
      "(2, 64, 114, 32)\n",
      "Shape of created feature maps after zero padding:\n",
      "(2, 70, 120, 32)\n",
      "Shape of kernels:\n",
      "(7, 7, 3, 32)\n",
      "Shape of biases:\n",
      "(32,)\n",
      "Output of global average 2D layer:\n",
      "tf.Tensor(\n",
      "[[0.64338624 0.5971759  0.5824972 ]\n",
      " [0.76306933 0.26011038 0.10849128]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_sample_images\n",
    "\n",
    "images=load_sample_images()['images']\n",
    "print('Length of images:')\n",
    "print(len(images))\n",
    "print('Shape of images:')\n",
    "print(np.shape(images))\n",
    "#there are 2 images here, each image has dimensions 427x640, there are 3 color channels(rgb)\n",
    "\n",
    "images=tf.keras.layers.CenterCrop(height=70,width=120)(images)\n",
    "#retains only center patch of each image\n",
    "images=tf.keras.layers.Rescaling(scale=1/255)(images)\n",
    "#this rescales every pixel value to 0-1 range by dividing them by 255\n",
    "#coloe and intensity of pixels remain same as relative diff between pixel values is retained\n",
    "print('Shape of images after preprocessing:')\n",
    "print(images.shape)\n",
    "\n",
    "conv_layer=tf.keras.layers.Conv2D(filters=32,kernel_size=7) #32 different feature maps created\n",
    "fmaps=conv_layer(images)\n",
    "print('Shape of created feature maps before zero padding:')\n",
    "print(fmaps.shape)\n",
    "#width and height changes because we lose pixels due to lack of zero padding\n",
    "\n",
    "conv_layer=tf.keras.layers.Conv2D(filters=32,kernel_size=7, padding='same') #32 different feature maps created\n",
    "fmaps=conv_layer(images)\n",
    "print('Shape of created feature maps after zero padding:')\n",
    "print(fmaps.shape)\n",
    "\n",
    "kernels,biases=conv_layer.get_weights()\n",
    "print('Shape of kernels:')\n",
    "print(kernels.shape)\n",
    "#kernels are the filters of each convolutional layer, store the weights of the filters\n",
    "#7,7 is size of kernel, 3 represents number of color channels, 32 represents number of filters\n",
    "print('Shape of biases:')\n",
    "print(biases.shape)\n",
    "#every filter has a bias term\n",
    "\n",
    "#POOLING LAYERS:\n",
    "max_pool=tf.keras.layers.MaxPool2D(pool_size=2)\n",
    "#you can also use AvgPool2D for average pooling layer\n",
    "global_avg_pool=tf.keras.layers.GlobalAvgPool2D()\n",
    "#this layer computes mean of entire feature map and outputs only 1 value per color channel(no receptive field logic)\n",
    "\n",
    "global_res=global_avg_pool(images)\n",
    "print('Output of global average 2D layer:')\n",
    "print(global_res)#2 lists for 2 images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f5d5ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 321s 213ms/step - loss: nan - accuracy: 0.1000 - val_loss: nan - val_accuracy: 0.1002\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 316s 211ms/step - loss: nan - accuracy: 0.1000 - val_loss: nan - val_accuracy: 0.1002\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 309s 206ms/step - loss: nan - accuracy: 0.1000 - val_loss: nan - val_accuracy: 0.1002\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 311s 207ms/step - loss: nan - accuracy: 0.1000 - val_loss: nan - val_accuracy: 0.1002\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 313s 208ms/step - loss: nan - accuracy: 0.1000 - val_loss: nan - val_accuracy: 0.1002\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 326s 217ms/step - loss: nan - accuracy: 0.1000 - val_loss: nan - val_accuracy: 0.1002\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 327s 218ms/step - loss: nan - accuracy: 0.1000 - val_loss: nan - val_accuracy: 0.1002\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 323s 215ms/step - loss: nan - accuracy: 0.1000 - val_loss: nan - val_accuracy: 0.1002\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 316s 211ms/step - loss: nan - accuracy: 0.1000 - val_loss: nan - val_accuracy: 0.1002\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 315s 210ms/step - loss: nan - accuracy: 0.1000 - val_loss: nan - val_accuracy: 0.1002\n",
      "Evaluation of this model on test data:\n",
      "313/313 [==============================] - 22s 70ms/step - loss: nan - accuracy: 0.1000\n",
      "[nan, 0.10000000149011612]\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "(train_images,train_labels),(test_images,test_labels)=fashion_mnist.load_data()\n",
    "\n",
    "train_images,val_images,train_labels,val_labels=train_test_split(train_images,train_labels,test_size=0.2,random_state=42)\n",
    "train_images=np.expand_dims(train_images,axis=-1)\n",
    "test_images=np.expand_dims(test_images,axis=-1)\n",
    "val_images=np.expand_dims(val_images,axis=-1)\n",
    "\n",
    "#makes all images grayscale(1 color dimension)\n",
    "DefaultConv2D= partial(tf.keras.layers.Conv2D, kernel_size=3,padding='same',activation='relu', kernel_initializer='he_normal')\n",
    "model=tf.keras.Sequential([\n",
    "    DefaultConv2D(filters=64,kernel_size=7,input_shape=[28,28,1]), #specifies dimensions of input data it is working with\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    DefaultConv2D(filters=128),\n",
    "    DefaultConv2D(filters=128),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    DefaultConv2D(filters=256),\n",
    "    DefaultConv2D(filters=256),\n",
    "    tf.keras.layers.MaxPool2D(),#MaxPool2D works on every generated feature map\n",
    "    tf.keras.layers.Flatten(),# all diff feature maps due to filters concatenated together\n",
    "    tf.keras.layers.Dense(units=128,activation='relu',kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(units=64,activation='relu',kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(units=10,activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])\n",
    "history=model.fit(train_images,train_labels,validation_data=[val_images,val_labels],epochs=10)\n",
    "print('Evaluation of this model on test data:')\n",
    "print(model.evaluate(test_images,test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312a9947",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
